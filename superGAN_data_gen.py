import numpy as npimport h5pyimport picklefrom sklearn.preprocessing import OneHotEncoderimport osdef prepare_supergan_data(data_labels_path, data_specs_path, output_h5_path):    """    Prepare and save the dataset for SuperGAN in the required format without splitting into train/test.    This function closely matches the preprocessing steps used for the pre-trained model:    - Loads data from pickle files.    - Transposes data to (num_samples, seg_length, num_channels).    - Normalizes the data using min-max normalization per feature.    - One-hot encodes the labels.    - Updates the data specifications with the correct number of classes.    - Saves the processed data to an .h5 file with datasets 'X', 'y', and 'y_onehot'.    Args:        data_labels_path (str): Path to the data labels pickle file.        data_specs_path (str): Path to the data specifications pickle file.        output_h5_path (str): Path to save the output .h5 file.    """    try:        # Load data labels        with open(data_labels_path, 'rb') as file:            data_dict = pickle.load(file)        data = data_dict['data']  # Expected Shape: (n_window, n_channel, n_data)        labels_array = np.array(data_dict['labels'])  # Expected Shape: (n_window,)        print(f"Loaded data labels with shape: {data.shape} and labels shape: {labels_array.shape}")    except FileNotFoundError:        print(f"Error: The file {data_labels_path} was not found.")        return    except Exception as e:        print(f"An error occurred while loading {data_labels_path}: {e}")        return    try:        # Load data specifications        with open(data_specs_path, 'rb') as file:            dataSpecs = pickle.load(file)        SEG_SIZE = dataSpecs.get('SEG_SIZE', 206)  # Default to 206 if not specified        CHANNEL_NB = dataSpecs.get('CHANNEL_NB', 3)  # Default to 3 if not specified        # Automatically set CLASS_NB based on unique labels        unique_classes = np.unique(labels_array)        CLASS_NB = len(unique_classes)        dataSpecs['CLASS_NB'] = CLASS_NB  # Update CLASS_NB in dataSpecs        print(f"Data specifications updated: SEG_SIZE={SEG_SIZE}, CHANNEL_NB={CHANNEL_NB}, CLASS_NB={CLASS_NB}")    except FileNotFoundError:        print(f"Error: The file {data_specs_path} was not found.")        return    except Exception as e:        print(f"An error occurred while loading {data_specs_path}: {e}")        return    # Validate data dimensions    n_window, n_channel, n_data = data.shape    if n_channel != CHANNEL_NB:        print(f"Warning: Expected {CHANNEL_NB} channels, but got {n_channel} channels.")    if n_data != SEG_SIZE:        print(f"Warning: Expected segment size {SEG_SIZE}, but got {n_data}.")    # Reshape the data for SuperGAN input    # Required shape: (num_samples, seg_length, num_channels)    X = data.transpose(0, 2, 1)  # Shape: (n_window, seg_length, num_channels)    print(f"Reshaped X to: {X.shape}")    # Normalize the data (min-max normalization per feature)    epsilon = 1e-8  # To prevent division by zero    # Compute min and max per sample and channel    X_min = X.min(axis=1, keepdims=True)  # Shape: (n_window, 1, num_channels)    X_max = X.max(axis=1, keepdims=True)  # Shape: (n_window, 1, num_channels)    X_normalized = (X - X_min) / (X_max - X_min + epsilon)    print("Applied min-max normalization to X.")    # One-hot encode the labels    encoder = OneHotEncoder(sparse_output=False)  # Updated parameter    y_onehot = encoder.fit_transform(labels_array.reshape(-1, 1))  # Shape: (n_window, num_classes)    print(f"One-hot encoded labels to shape: {y_onehot.shape}")    # Ensure the output directory exists    output_dir = os.path.dirname(output_h5_path)    if output_dir and not os.path.exists(output_dir):        os.makedirs(output_dir)        print(f"Created directory: {output_dir}")    # Save the updated dataSpecs.pkl with the correct CLASS_NB    try:        with open(data_specs_path, 'wb') as file:            pickle.dump(dataSpecs, file)        print(f"Updated data specifications saved to {data_specs_path}.")    except Exception as e:        print(f"An error occurred while updating {data_specs_path}: {e}")        return    # Save the data to an .h5 file    try:        with h5py.File(output_h5_path, 'w') as h5file:            h5file.create_dataset('X', data=X_normalized.astype(np.float32))        # float32            h5file.create_dataset('y', data=labels_array.astype(np.int64))          # int64            h5file.create_dataset('y_onehot', data=y_onehot.astype(np.float32))     # float32        print(f"Data successfully prepared and saved to {output_h5_path}.")    except Exception as e:        print(f"An error occurred while saving to {output_h5_path}: {e}")        return    # Validate the saved .h5 file    try:        with h5py.File(output_h5_path, 'r') as h5file:            print("Datasets in the .h5 file:")            for key in h5file.keys():                print(f"- {key}: shape = {h5file[key].shape}, dtype = {h5file[key].dtype}")            # Access the data            X_loaded = h5file['X'][:]            y_loaded = h5file['y'][:]            y_onehot_loaded = h5file['y_onehot'][:]            print(f"Loaded X shape: {X_loaded.shape}, y shape: {y_loaded.shape}, y_onehot shape: {y_onehot_loaded.shape}")    except Exception as e:        print(f"An error occurred while validating the .h5 file: {e}")        return# Example usage:if __name__ == "__main__":    data_labels_path = "/Users/Lerberber/Desktop/Bhat/SuperGans/CNN1_scripts/Datasets/Epilepsy_dataLabels.pkl"    data_specs_path = "/Users/Lerberber/Desktop/Bhat/SuperGans/CNN1_scripts/Datasets/Epilepsy_specs.pkl"    output_h5_path = "/Users/Lerberber/Desktop/Bhat/SuperGans/supergan_data.h5"    prepare_supergan_data(data_labels_path, data_specs_path, output_h5_path)